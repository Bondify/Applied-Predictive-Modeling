---
title: "Applied Predictive Modeling"
subtitle: "Chapter 3: Data Pre-Processing"
author: "Santiago Toso"
output:
  html_document:
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_float: yes
---

# Data pre-procesing

In this notebooks we will follow the computing in examples of the book [Applied Predictive Modelling](http://appliedpredictivemodeling.com/) by Max Kuhn and Kjell Johnson.
Some of the commands will be exactly the ones you find on the book. Others, could be modified but always getting the same results.


```{r}
#install.packages('AppliedPredictiveModeling')
#install.packages('caret')
library('AppliedPredictiveModeling')
library(tidyverse)
library(pryr)
library(dplyr)
library(ggplot2)
library(reshape2)
library(knitr)
```

We'll use the sample data the author shows during this chapter. This data comes from the library `AppliedPredictiveModeling` that we uploaded below.
```{r}
data("segmentationOriginal")
```

The variable `Case` indicates if that specific sample was part of the training or testing set. We'll only keep the ones for training right now.

```{r}
segData <- segmentationOriginal %>% 
  filter(Case == 'Train')
kable(as.tibble(head(segData)[1:10]))
```

The `Class` and `Cell` fields will be saved in their own vector, then removed from the main object:
```{r}
cellID <- segData$Cell
clase <- segData$Class
case <- segData$Case

segData <- segData[, -(1:3)]
```

The original data contained several "status" columns which were binary versions of the predictors. To remove these, we find the column names containing "Status" and remove them:
```{r}
statusColNum <- grep("Status", names(segData))
statusColNum
segData <- segData[, -statusColNum]
```

## Data Transformations 

### Skewness

We are going to treat the skewness of these predictors with the `skewness` function of the library `e1071`.

```{r}
library(e1071)
```

We will see first what kind of variables we are dealing with in this data frame.

```{r}
kable(unique(lapply(segData, class)))
```

Since all the predictors are numeric or integer we can apply the `skewness` function to all of them.

```{r}
skewValues <- apply(segData, 2, skewness)
head(skewValues)
```

We can see that some of the variables have considerable skewness (numbers different from zero). Taking these variables as an example we could make an histogram of each of them to visually see the skewness.

```{r message=FALSE, warning=FALSE}
sixVar <- melt(segData[,1:6])
ggplot(sixVar, aes(value)) + facet_wrap(~variable, scales = 'free_x') +
  geom_area(stat = 'bin', binwidth = function(x) 2 * IQR(x) / (length(x)^(1/3)), fill = '#41b6c4') +
  # If we wanted to see histograms or frequency curves instead of the area graph we could use the next lines
  # geom_histogram(binwidth = function(x) 2 * IQR(x) / (length(x)^(1/3)), fill = '#41b6c4') +
  # geom_freqpoly(binwidth = function(x) 2 * IQR(x) / (length(x)^(1/3)), color = '#41b6c4') +
  theme(panel.grid = element_blank(),
        panel.background = element_blank(),
        axis.line.x = element_line(colour = "grey"),
        axis.line.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.x = element_line(colour = "grey"),
        axis.ticks.y = element_blank()
        )
```

We can specify a function for calculating binwidth, particularly useful when faceting along variables with different ranges as it is the case for us.
In this particular case, we make the binwidth a function of the Inter-Quartile range of the variable (function `IQR`) and the number of samples.

Visually, it is pretty easy to notice the high skewness calculated in the step before.

Now, how can we deal with this? The authors proposed to use the Box-Cox method to determine what kind of transformation we should use. They propose the `BoxCoxTrans` function from the `caret` package to estimate λ for each of the variable and directly apply the correspondig transformation.

```{r}
library(caret)
Ch1AreaTrans <- BoxCoxTrans(segData$AreaCh1)
Ch1AreaTrans
```

We can compare the original data set with the transformed one:

```{r}
kable(head(segData$AreaCh1))
```

After the transformation:

```{r}
kable(predict(Ch1AreaTrans, head(segData$AreaCh1)))
```

If we apply the Box-Cox formula with the `λ = -0.9` we can check the transformation.

```{r}
kable((head(segData$AreaCh1)^(-.9) - 1) / (-.9))
```

### Centereing, Scaling and using PCA

To complete the transformations we need to do to our data, the authors propose to use another `caret` function, `preProcess`, that allows us to apply more than one transformation to a set of predictors. This function will be discussed in a minute. 

First, we are going to center and scale the data. Then, we'll apply the `prcomp` function to apply **PCA** to our data.

```{r}
pcaObject <- prcomp(segData,
                    center = TRUE,
                    scale. = TRUE)
```

We could check the cumulative percentage of variance which each component accounts for and then make a graph of it.

```{r}
percentVariance <- (pcaObject$sdev^2) / sum(pcaObject$sdev^2) * 100
percentVariance[1:3]
```

We can also put it graphically. We create a line in the 6th variable since it is that is the last variable the PCA method considered.

```{r}
dfPercentVariance<- as.data.frame(percentVariance)

dfPercentVariance <- dfPercentVariance %>% 
  mutate(comp = c(1:length(percentVariance)))

ggplot(data = dfPercentVariance, aes(x = comp, y = percentVariance)) +
  geom_point(color = '#41b6c4') + 
  geom_line(color = '#41b6c4') +
  theme(panel.grid = element_blank(),
        panel.background = element_blank(),
        axis.line = element_line(colour = "grey"),
        axis.ticks = element_line(colour = "grey")
        ) +
   annotate("segment", x = 6, xend = 6, y = 0, yend = 20, colour = "red")
```

The transformed values are stored in our object `pcaObject` as the sub-object called `x`:

```{r}
head(pcaObject$x[,1:5])
```

The other sub-object called `rotation` tells us the varible loadings:

```{r}
head(pcaObject$rotation[,1:3])
```

The author points out that if we wanted to use the functionality for the *spatial sign* transformation we could get it in the class `spatialSign` that is included in the `caret` package. We will not apply it now, but the sintax would simpy be `spatialSign(segData)`.

Also, these data has no missing values, which means we won't be able to practice the values imputation. Nonetheless, the authors point out that we could apply the **K-Nearest neighbor** method to it using the `impute.knn` functionality from the `imput` package. The previously mentioned `preProcess` function applies imputation methods based on K-nearest neighbor or bagged trees.

To administer a series of transformations to multiple data sets (or variables) the `caret` class `preProcess` has the ability to transform, center, scale or impute values, as well as apply the spatial sign transformation and feature extraction. The function calculates the required quantities for the transformation. After calling the `preProcess` function, the `predict` method applies the results to a set of data. For example, Box-Cox transform, center and scale the data, then execute PCA for signal extraction, the syntax would be:

```{r}
trans <- preProcess(segData,
                    method = c('BoxCox', "center", "scale", "pca"))
trans
```

Now we can apply the transformations.

```{r}
transformed <- predict(trans, segData)
head(transformed[,1:5])
```

The order in which the transformations are applied is transformation, centering, scaling, imputation, feature extracton and then spatial sign.
 
Many modeling functions have options to center and scale prior to modeling. For example, when using the `train` function (discussed in later chapters), there is an option to use `preProcess` prior to modeling within the resampling iterations.

## Filtering

The author tells us that to filter for near-zero variance predictors, the `caret` package function `nearZeroVar` will return the column numbers of any predictors that fulfill these conditions:

- The fraction of unique values over the sample size is < 10%.
- The ratio of the frequency of the most prevalent value to the frequency of the second most prevalent value is large (~20).

For our data, it looks that none of the variables are problematic.

```{r}
nearZeroVar(segData)
```

In case we had some problematic predictors, the function would return a vector of integers with the column numbers of the predictors that should be removed.

Similarly, to filter the on betwee-predictor correlations the `cor` function can calculate the correlations between predictor variables:

```{r}
correlations <- cor(segData)
correlations[1:4, 1:4]
```

To visually examine the correlations structure of the data the `corrplot` package contains an excellent function of the same name. The function has many options including one that will reorder the variables in a way that reveals clusters of highly correlated predictors. The following command can help.

```{r}
#install.packages('corrplot')
library(corrplot)
corrplot(correlations, order = 'hclust', tl.cex = 0.4)
```

The size and color of the points is associated with the strenght of correlations between two predictor variables.

To filter based on correlations the `findCorrelation` function will apply the algorithm explained by the authors in Sect. 3.5:

1- Calculate the correlation matrix of the predictors.
2- Determine the 2 predictors with the higher correlation (call them A and B).
3- Determine the average correlation between A and the rest of the predictors. Do the same for B.
4- Remove the predictor with the highest average correlation.
5- Repeat steps 2-4 until all absolute correlations are above the threshold defined.

For a given threshold of pairwise correlations, the function returns column numbers denoting the predictors recommended for deletion.

```{r}
highCorr <- findCorrelation(correlations, cutoff = .75)
length(highCorr)
```

The algorithm recommends us to remove 32 variables in order to respect the threshold.

```{r}
filteredSegData <- segData[, -highCorr]
```

## Creating Dummy variables

Will leave this subject for section 4.9 where several methods to create dummy variables are introduced.








